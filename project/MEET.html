<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>MEET</title>
	<link type="text/css" href="/project/head/system.css" rel="stylesheet"/>
	<link type="text/css" href="/project/head/custom.css" rel="stylesheet"/>
	<script type="text/javascript" src="/project/head/custom.js"></script>


</head>

<body marginheight="0">
<div align="center"><h1>MEET: A Million-Scale Dataset for Fine-Grained Geospatial Scene Classification with Zoom-Free Remote Sensing Imagery<br></h1></div>

<!--<p style="text-align: center;">Yansheng Li, Yuning Wu</p> -->


	<!--
<div style="border: 18px solid #FFFFFF"></div>
<p style="font-size: 12px; color: orange; text-align: center">The <b>Five-Billion-Pixels</b> dataset is released!</p>
        -->

<p style="text-align: center;">
	<a href><b>[Paper]</b></a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;

	<a href><b>[Github]</b></a>
</p>

<img src="/figure/0807_intro.jpg" width="800px">


<!--<h3>‚≠êÔ∏è Highlights </h3>-->
<!--<p> STAR, the first large-scale dataset  for OBD and SGG in <b> large-size </b> VHR SAI. <p>-->

<h3>Introduction</h3>
Accurate fine-grained geospatial scene classification with remote sensing imagery is critical in various applications. Existing research manually zooms remote sensing images with different rates to form typical scene samples, which cannot well support the fixed-resolution image interpretation requirement in actual applications. To address this issue, we introduce a Million-scale finE-grained geospatial scEne classification dataseT (MEET), which consists of over 1.03 million zoom-free remote sensing image scene samples, which are manually annotated into 80 fine-grained categories. In MEET, each scene sample adopts a scene-in-scene layout where the center scene is the reference scene and the auxiliary scenes provide essential spatial contexts for fine-grained classification of the center scene. To tackle with the emerging scene-in-scene classification challenge, this paper presents a novel Context-Aware Transformer (CAT) that can leverage adaptive fusion of spatial contexts to elaborately classify the scene sample by learning the attentional feature between the center scene and the auxiliary scenes. Based on MEET, we establish a fine-grained geospatial scene classification benchmark with 12 fine-grained geospatial scene classification methods and extensive experiments validate the effectiveness of CAT in terms of the comparison with competitive baselines and the ablation study.






<h3>MEET Dataset</h3>
<p> MEET dataset is comprised of over 1.03 million sample annotation pairs, encompassing 80 fine-grained scene categories. Samples are collected globally and include multi-level spatial context information. The large sample size, the granularity of categories, and the inclusion of spatial context imagery make MEET a valuable dataset.	<br>
MEET dataset has five remarkable and important advantages:

Fine Granularity of Categories
Large Volume of Samples
High Intra-class Variability and Inter-class Similarity




<style>
	.tabMenu ul {
		display: flex;
		flex-wrap: nowrap;
		overflow-x: auto;
		padding: 0;
		margin: 0;
		align-items: center;
	}

	.tabMenu ul li {
		white-space: nowrap;
	}
</style>



<div style="border: 9px solid #FFFFFF"></div>
<div id = "tab1" class = "tabMenu">
	<ul>
	<li class="on"><h4>Dataset Overview</h4></li>
	<li class="off"><h4>Distribution</h4></li>
        <li class="off"><h4>Statistics</h4></li>
        </ul>
	<div id="firstPage-tab1" class="show">
	<img src="/figure/0720_allcase.png" width="800px">
        </div>

		<div id="thirdPage-tab1" class="hide">
			<img src="/figure/0720_distribute.png" width="800px">
	
			</div>

        <div id="secondPage-tab1" class= "hide">
	    <img src="/figure/0720_longtail.jpg" width="800px">
      	</div>

</div>
<div style="border: 9px solid #FFFFFF"></div>

<h3>CAT</h3>
<p>  We introduce CAT, a specialized framework to optimize the utilization of both the primary image sample and its corresponding contextual information. <p>
<img src="/figure/0720_method.png" width="800px">

<img src="/figure/cat_result.png" width="800px">




<h3>Generalization experiments on MEET</h3>


<style>
	.tabMenu ul {
		display: flex;
		flex-wrap: nowrap;
		overflow-x: auto;
		padding: 0;
		margin: 0;
		align-items: center;
	}

	.tabMenu ul li {
		white-space: nowrap;
	}
</style>

<img src="/figure/ufz_result.png" width="800px">

<div style="border: 9px solid #FFFFFF"></div>
<div id = "tab2" class = "tabMenu">
	<ul>
	<li class="on"><h4>Shanghai</h4></li>
        <li class="off"><h4>Wuhan</h4></li>
        </ul>
	<div id="firstPage-tab1" class="show">
	<img src="/figure/web_utf1.png" width="800px">
        </div>

        <div id="secondPage-tab1" class= "hide">
	    <img src="/figure/web_utf2.png" width="800px">
      	</div>

</div>
<div style="border: 9px solid #FFFFFF"></div>














<!--
<h3>Acknowledge</h3>


<h2 id="citation">Citation</h2>
<p>If you find this work helpful for your research, please consider citing our papers and staring ‚≠ê<b><a href target="_blank" rel="noopener noreferrer" style="color: orange;">CAT</a></b>:</p>
<pre>
@article{li2024scene,
    title={STAR: A First-Ever Dataset and A Large-Scale Benchmark for Scene Graph Generation in Large-Size Satellite Imagery},
    author={Li, Yansheng and Wang, Linlin and Wang, Tingzhu and Yang, Xue and Luo, Junwei and Wang, Qi and Deng, Youming and Wang, Wenbin and Sun, Xian and Li, Haifeng and Dang, Bo and Zhang, Yongjun and Yu, Yi and Yan Junchi},
    journal={arXiv preprint arXiv:2406.09410},
    year={2024}}

@article{li2024fine,
  title={Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction},
  author={Li, Yansheng and Wang, Tingzhu and Wu, Kang and Wang, Linlin and Guo, Xin and Wang, Wenbin},
  journal={arXiv preprint arXiv:2407.19259},
  year={2024}
}

@article{luo2024sky,
    title={SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding},
    author={Luo, Junwei and Pang, Zhen and Zhang, Yongjun and Wang, Tingzhu and Wang, Linlin and Dang, Bo and Lao, Jiangwei and Wang, Jian and Chen, Jingdong and Tan, Yihua and Li, Yansheng},
    journal={arXiv preprint arXiv:2406.10100},
    year={2024}}

@article{li2024learning,
    title={Learning to Holistically Detect Bridges From Large-Size VHR Remote Sensing Imagery},
    author={Li, Yansheng and Luo, Junwei and Zhang, Yongjun and Tan, Yihua and Yu, Jin-Gang and Bai, Song},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume={44},
    number={11},
    pages={7778--7796},
    year={2024},
    publisher={IEEE}}

@inproceedings{deng2022hierarchical,
    title={Hierarchical Memory Learning for Fine-grained Scene Graph Generation},
    author={Deng, Youming and Li, Yansheng and Zhang, Yongjun and Xiang, Xiang and Wang, Jian and Chen, Jingdong and Ma, Jiayi},
    booktitle={European Conference on Computer Vision},
    pages={266--283},
    year={2022},
    organization={Springer}}

</pre>



<h3>Contact</h3>
<p>E-mail: yansheng.li@whu.edu.cn; yuning.wu@whu.edu.cn</p>

<span style="vertical-align: middle;">üîç <b>Real-time views of the web page MEET are</b> </span>
<a href="https://info.flagcounter.com/LqCH">
    <img src="https://s01.flagcounter.com/mini/LqCH/bg_F2F2F2/txt_FF8C00/border_CCCCCC/flags_0/" alt="Free counters!" style="vertical-align: middle;" border="0">
</a>
-->



</body>
</html>
