<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>STAR</title>
	<link type="text/css" href="/project/head/system.css" rel="stylesheet"/>
	<link type="text/css" href="/project/head/custom.css" rel="stylesheet"/>
	<script type="text/javascript" src="/project/head/custom.js"></script>

	<!-- <style>
		body {
			background-image: url("/figure/rs_distri.jpg");
			background-repeat: no-repeat;
			background-size: 800px; /* 
		} -->
	<!-- </style> -->

</head>

<body marginheight="0">
<div align="center"><h1>Fine-Grained Geospatial Scene Classification: A Million-Scale Dataset and A Context-Aware Transformer<br></h1></div>

<p style="text-align: center;">Yansheng Li, Yuning Wu</p>

<!-- <p style="text-align: center">Yansheng Li, Linlin Wang, Tingzhu Wang, Qi Wang, Xian Sun, Xue Yang, Wenbin Wang, Junwei Luo, Youming Deng, Haifeng Li, Bo Dang, Yongjun Zhang, Junchi Yan</p> -->

	<!--
<div style="border: 18px solid #FFFFFF"></div>
<p style="font-size: 12px; color: orange; text-align: center">The <b>Five-Billion-Pixels</b> dataset is released!</p>
        -->

<p style="text-align: center;">
	<a href="https://arxiv.org/abs/2406.09410"><b>[Paper]</b></a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;

	<a href="https://github.com/Zhuzi24/SGG-ToolKit"><b>[Github]</b></a>
</p>

<img src="/figure/0807_intro.jpg" width="800px">


<h3>‚≠êÔ∏è Highlights </h3>
<!--<p> STAR, the first large-scale dataset  for OBD and SGG in <b> large-size </b> VHR SAI. <p>-->

<h3>MEET Dataset</h3>
<p> MEET dataset is comprised of over 1.03 million sample annotation pairs, encompassing 80 fine-grained scene categories. Samples are collected globally and include multi-level spatial context information. The large sample size, the granularity of categories, and the inclusion of spatial context imagery make MEET a valuable dataset.	<br>


	<img src="/figure/0720_distribute.png" width="800px">



<style>
	.tabMenu ul {
		display: flex;
		flex-wrap: nowrap;
		overflow-x: auto;
		padding: 0;
		margin: 0;
		align-items: center;
	}

	.tabMenu ul li {
		white-space: nowrap;
	}
</style>



<div style="border: 9px solid #FFFFFF"></div>
<div id = "tab1" class = "tabMenu">
	<ul>
	<li class="on"><h4>Dataset Overview</h4></li>
        <li class="off"><h4>Statistics</h4></li>
		<li class="off"><h4>Hierarchical scene category</h4></li>
        </ul>
	<div id="firstPage-tab1" class="show">
	<img src="/figure/0720_allcase.png" width="800px">
        </div>

        <div id="secondPage-tab1" class= "hide">
	    <img src="/figure/0720_longtail.jpg" width="800px">
      	</div>

		<div id="thirdPage-tab1" class="hide">
		<img src="/figure/0720_sun_class.png" width="800px">

		</div>
</div>
<div style="border: 9px solid #FFFFFF"></div>

<h3>CAT</h3>
<p>  We introduce CAT, a specialized framework to optimize the utilization of both the primary image sample and its corresponding contextual information. <p>
<img src="/figure/0720_method.png" width="800px">


<h3>Acknowledge</h3>
<!-- <p>This work was supported by the National Natural Science Foundation of China.</p>-->


<h2 id="citation">Citation</h2>
<p>If you find this work helpful for your research, please consider citing our papers and staring ‚≠ê<b><a href="https://github.com/Zhuzi24/SGG-ToolKit" target="_blank" rel="noopener noreferrer" style="color: orange;">SAI-oriented SGG toolkit</a></b>:</p>
<pre>
@article{li2024scene,
    title={STAR: A First-Ever Dataset and A Large-Scale Benchmark for Scene Graph Generation in Large-Size Satellite Imagery},
    author={Li, Yansheng and Wang, Linlin and Wang, Tingzhu and Yang, Xue and Luo, Junwei and Wang, Qi and Deng, Youming and Wang, Wenbin and Sun, Xian and Li, Haifeng and Dang, Bo and Zhang, Yongjun and Yu, Yi and Yan Junchi},
    journal={arXiv preprint arXiv:2406.09410},
    year={2024}}

@article{li2024fine,
  title={Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction},
  author={Li, Yansheng and Wang, Tingzhu and Wu, Kang and Wang, Linlin and Guo, Xin and Wang, Wenbin},
  journal={arXiv preprint arXiv:2407.19259},
  year={2024}
}

@article{luo2024sky,
    title={SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding},
    author={Luo, Junwei and Pang, Zhen and Zhang, Yongjun and Wang, Tingzhu and Wang, Linlin and Dang, Bo and Lao, Jiangwei and Wang, Jian and Chen, Jingdong and Tan, Yihua and Li, Yansheng},
    journal={arXiv preprint arXiv:2406.10100},
    year={2024}}

@article{li2024learning,
    title={Learning to Holistically Detect Bridges From Large-Size VHR Remote Sensing Imagery},
    author={Li, Yansheng and Luo, Junwei and Zhang, Yongjun and Tan, Yihua and Yu, Jin-Gang and Bai, Song},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume={44},
    number={11},
    pages={7778--7796},
    year={2024},
    publisher={IEEE}}

@inproceedings{deng2022hierarchical,
    title={Hierarchical Memory Learning for Fine-grained Scene Graph Generation},
    author={Deng, Youming and Li, Yansheng and Zhang, Yongjun and Xiang, Xiang and Wang, Jian and Chen, Jingdong and Ma, Jiayi},
    booktitle={European Conference on Computer Vision},
    pages={266--283},
    year={2022},
    organization={Springer}}

</pre>



<h3>Contact</h3>
<p>E-mail: yansheng.li@whu.edu.cn; yuning.wu@whu.edu.cn</p>

<span style="vertical-align: middle;">üîç <b>Real-time views of the web page MEET are</b> </span>
<a href="https://info.flagcounter.com/LqCH">
    <img src="https://s01.flagcounter.com/mini/LqCH/bg_F2F2F2/txt_FF8C00/border_CCCCCC/flags_0/" alt="Free counters!" style="vertical-align: middle;" border="0">
</a>

</body>
</html>
